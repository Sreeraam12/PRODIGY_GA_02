# PRODIGY_GA_02
Image Generation with Pre-trained Models

# Task-2: Image Generation with Pre-trained Models

In this task, I explored **text-to-image generation** using pre-trained generative models such as **DALLÂ·E-mini** and **Stable Diffusion**. The objective was to generate images from natural language prompts using powerful generative AI tools.

## ðŸ” Objectives
- Understand the basics of text-to-image synthesis
- Use pre-trained models like **DALLÂ·E-mini** and **Stable Diffusion**
- Generate creative visuals from descriptive prompts

## ðŸ› ï¸ Tools & Libraries
- Python
- Hugging Face Datasets & Transformers
- diffusers library (for Stable Diffusion)
- Google Colab

## ðŸ“ Contents
- `image_generation.ipynb` â€“ Notebook for text-to-image generation
- `prompts.txt` â€“ List of prompts used
- `generated_images/` â€“ Folder containing sample generated images

## ðŸš€ How to Run
1. Open `image_generation.ipynb` in Google Colab.
2. Install required libraries (e.g., `diffusers`, `transformers`, `torch`).
3. Provide text prompts.
4. Run the cells to generate and view images.

## ðŸŽ¨ Sample Prompt & Output
> **Prompt:** *"A futuristic city at sunset with flying cars"*  
> *(Output image stored in `/generated_images/`)*

## ðŸ“Œ Outcome
Successfully generated images from custom prompts using state-of-the-art generative models, demonstrating how AI can convert imagination into visuals.

---

