# PRODIGY_GA_02
Image Generation with Pre-trained Models

# Task-2: Image Generation with Pre-trained Models

In this task, I explored **text-to-image generation** using pre-trained generative models such as **DALL·E-mini** and **Stable Diffusion**. The objective was to generate images from natural language prompts using powerful generative AI tools.

## 🔍 Objectives
- Understand the basics of text-to-image synthesis
- Use pre-trained models like **DALL·E-mini** and **Stable Diffusion**
- Generate creative visuals from descriptive prompts

## 🛠️ Tools & Libraries
- Python
- Hugging Face Datasets & Transformers
- diffusers library (for Stable Diffusion)
- Google Colab

## 📁 Contents
- `image_generation.ipynb` – Notebook for text-to-image generation
- `prompts.txt` – List of prompts used
- `generated_images/` – Folder containing sample generated images

## 🚀 How to Run
1. Open `image_generation.ipynb` in Google Colab.
2. Install required libraries (e.g., `diffusers`, `transformers`, `torch`).
3. Provide text prompts.
4. Run the cells to generate and view images.

## 🎨 Sample Prompt & Output
> **Prompt:** *"A futuristic city at sunset with flying cars"*  
> *(Output image stored in `/generated_images/`)*

## 📌 Outcome
Successfully generated images from custom prompts using state-of-the-art generative models, demonstrating how AI can convert imagination into visuals.

---

